{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46122f2b",
   "metadata": {
    "papermill": {
     "duration": 0.005684,
     "end_time": "2025-11-22T11:41:31.004171",
     "exception": false,
     "start_time": "2025-11-22T11:41:30.998487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Student Information\n",
    "* Name: Yaqian Huang\n",
    "* ID: 20223801060"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf212218",
   "metadata": {
    "papermill": {
     "duration": 0.004644,
     "end_time": "2025-11-22T11:41:31.013695",
     "exception": false,
     "start_time": "2025-11-22T11:41:31.009051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3bb5b9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:41:31.025347Z",
     "iopub.status.busy": "2025-11-22T11:41:31.024694Z",
     "iopub.status.idle": "2025-11-22T11:42:16.568861Z",
     "shell.execute_reply": "2025-11-22T11:42:16.567889Z"
    },
    "papermill": {
     "duration": 45.557432,
     "end_time": "2025-11-22T11:42:16.576024",
     "exception": false,
     "start_time": "2025-11-22T11:41:31.018592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.2.0) (3.13.1)\r\n",
      "Installing collected packages: triton\r\n",
      "Successfully installed triton-2.2.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06843c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:42:16.587670Z",
     "iopub.status.busy": "2025-11-22T11:42:16.587412Z",
     "iopub.status.idle": "2025-11-22T11:43:04.200880Z",
     "shell.execute_reply": "2025-11-22T11:43:04.199883Z"
    },
    "papermill": {
     "duration": 47.626965,
     "end_time": "2025-11-22T11:43:04.208101",
     "exception": false,
     "start_time": "2025-11-22T11:42:16.581136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->xformers==0.0.24042abc8.d20240802) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.3.0)\r\n",
      "Installing collected packages: xformers\r\n",
      "Successfully installed xformers-0.0.24+042abc8.d20240802\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e1441c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:04.220862Z",
     "iopub.status.busy": "2025-11-22T11:43:04.220620Z",
     "iopub.status.idle": "2025-11-22T11:43:45.332460Z",
     "shell.execute_reply": "2025-11-22T11:43:45.331444Z"
    },
    "papermill": {
     "duration": 41.127627,
     "end_time": "2025-11-22T11:43:45.341150",
     "exception": false,
     "start_time": "2025-11-22T11:43:04.213523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: faiss-cpu\r\n",
      "Successfully installed faiss-cpu-1.7.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dda4781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:45.354042Z",
     "iopub.status.busy": "2025-11-22T11:43:45.353544Z",
     "iopub.status.idle": "2025-11-22T11:43:54.290631Z",
     "shell.execute_reply": "2025-11-22T11:43:54.289626Z"
    },
    "papermill": {
     "duration": 8.945873,
     "end_time": "2025-11-22T11:43:54.292598",
     "exception": false,
     "start_time": "2025-11-22T11:43:45.346725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/some-pack/sentence_transformers_packages\r\n",
      "Processing /kaggle/input/some-pack/sentence_transformers_packages/sentence_transformers-3.3.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.42.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.4)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.7.4)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "Installing collected packages: sentence-transformers\r\n",
      "Successfully installed sentence-transformers-3.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-index --find-links=/kaggle/input/some-pack/sentence_transformers_packages sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe6cd40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:54.307256Z",
     "iopub.status.busy": "2025-11-22T11:43:54.306663Z",
     "iopub.status.idle": "2025-11-22T11:43:55.405578Z",
     "shell.execute_reply": "2025-11-22T11:43:55.404368Z"
    },
    "papermill": {
     "duration": 1.108732,
     "end_time": "2025-11-22T11:43:55.407796",
     "exception": false,
     "start_time": "2025-11-22T11:43:54.299064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/lmsys-modules-0805 human_pref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d362fc",
   "metadata": {
    "papermill": {
     "duration": 0.006267,
     "end_time": "2025-11-22T11:43:55.420479",
     "exception": false,
     "start_time": "2025-11-22T11:43:55.414212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d5ad11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:55.434330Z",
     "iopub.status.busy": "2025-11-22T11:43:55.434015Z",
     "iopub.status.idle": "2025-11-22T11:43:55.511660Z",
     "shell.execute_reply": "2025-11-22T11:43:55.510557Z"
    },
    "papermill": {
     "duration": 0.087186,
     "end_time": "2025-11-22T11:43:55.513987",
     "exception": false,
     "start_time": "2025-11-22T11:43:55.426801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/transformers-4.42.3-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/sympy-1.12.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/urllib3-2.2.2-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/tqdm-4.66.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/idna-3.7-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/jinja2-3.1.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/mpmath-1.3.0-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/networkx-3.3-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/filelock-3.15.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/certifi-2024.7.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/accelerate-0.32.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/packaging-24.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/requests-2.32.3-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/typing_extensions-4.12.2-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/huggingface_hub-0.23.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/fsspec-2024.6.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-modules-0805/utils.py\n",
      "/kaggle/input/lmsys-modules-0805/models/modeling_gemma2.py\n",
      "/kaggle/input/lmsys-modules-0805/models/modeling_llama.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/triton_utils.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/rms_norm.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/flash_attention_nopad.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/fused_rotary_emb.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/gelu_and_mul.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/silu_and_mul.py\n",
      "/kaggle/input/lmsys-modules-0805/data/collators.py\n",
      "/kaggle/input/lmsys-modules-0805/data/dataset.py\n",
      "/kaggle/input/lmsys-modules-0805/data/processors.py\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model.safetensors.index.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00003-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/config.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00001-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/tokenizer.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/tokenizer_config.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00004-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/special_tokens_map.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00002-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/tokenizer.model\n",
      "/kaggle/input/akemiiiiii/balanced_transformed_dataset.csv\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/spm.model\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/tokenizer.json\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/tokenizer_config.json\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/model_config.pth\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/special_tokens_map.json\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/custom_model_weights.pth\n",
      "/kaggle/input/akemiiiiii/custom_model_dir/added_tokens.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model.safetensors.index.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/config.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer_config.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/special_tokens_map.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer.model\n",
      "/kaggle/input/some-pack/balanced_transformed_dataset.csv\n",
      "/kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence-transformer-model/config.json\n",
      "/kaggle/input/some-pack/sentence-transformer-model/README.md\n",
      "/kaggle/input/some-pack/sentence-transformer-model/tokenizer.json\n",
      "/kaggle/input/some-pack/sentence-transformer-model/tokenizer_config.json\n",
      "/kaggle/input/some-pack/sentence-transformer-model/sentence_bert_config.json\n",
      "/kaggle/input/some-pack/sentence-transformer-model/config_sentence_transformers.json\n",
      "/kaggle/input/some-pack/sentence-transformer-model/model.safetensors\n",
      "/kaggle/input/some-pack/sentence-transformer-model/modules.json\n",
      "/kaggle/input/some-pack/sentence-transformer-model/special_tokens_map.json\n",
      "/kaggle/input/some-pack/sentence-transformer-model/vocab.txt\n",
      "/kaggle/input/some-pack/sentence-transformer-model/1_Pooling/config.json\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/certifi-2024.12.14-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/tqdm-4.67.1-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/sentence_transformers-3.3.1-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/jinja2-3.1.4-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/idna-3.10-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/mpmath-1.3.0-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/fsspec-2024.10.0-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/networkx-3.4.2-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/transformers-4.47.0-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/filelock-3.16.1-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/threadpoolctl-3.5.0-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/requests-2.32.3-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/joblib-1.4.2-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/urllib3-2.2.3-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/typing_extensions-4.12.2-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/sympy-1.13.1-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/huggingface_hub-0.26.5-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/packaging-24.2-py3-none-any.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/some-pack/sentence_transformers_packages/charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\n",
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model.safetensors.index.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00003-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/config.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00001-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/tokenizer.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/tokenizer_config.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00004-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/special_tokens_map.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00002-of-00004.safetensors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Walk through all directories and files under the '/kaggle/input' folder\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    # Loop through each file in the current directory\n",
    "    for filename in filenames:\n",
    "        # Print the full path to the file by joining the directory and filename\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f1da4",
   "metadata": {
    "papermill": {
     "duration": 0.00906,
     "end_time": "2025-11-22T11:43:55.530631",
     "exception": false,
     "start_time": "2025-11-22T11:43:55.521571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320c24e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:55.550932Z",
     "iopub.status.busy": "2025-11-22T11:43:55.550678Z",
     "iopub.status.idle": "2025-11-22T11:43:55.557194Z",
     "shell.execute_reply": "2025-11-22T11:43:55.556267Z"
    },
    "papermill": {
     "duration": 0.018532,
     "end_time": "2025-11-22T11:43:55.558879",
     "exception": false,
     "start_time": "2025-11-22T11:43:55.540347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original test CSV file from the Kaggle input directory\n",
    "df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n",
    "\n",
    "# Add dummy label columns indicating model A is always the winner\n",
    "df[\"winner_model_a\"] = 1\n",
    "df[\"winner_model_b\"] = 0\n",
    "df[\"winner_tie\"] = 0\n",
    "\n",
    "# Save the original test dataframe (with dummy labels) to a Parquet file\n",
    "df.to_parquet(\"test.parquet\", index=False)\n",
    "\n",
    "# Swap responses A and B — simulating the reverse comparison\n",
    "df[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\n",
    "\n",
    "# Save the swapped version to a separate Parquet file\n",
    "df.to_parquet(\"test_swap.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940cda34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:55.572854Z",
     "iopub.status.busy": "2025-11-22T11:43:55.572628Z",
     "iopub.status.idle": "2025-11-22T11:43:57.665886Z",
     "shell.execute_reply": "2025-11-22T11:43:57.664893Z"
    },
    "papermill": {
     "duration": 2.102198,
     "end_time": "2025-11-22T11:43:57.667746",
     "exception": false,
     "start_time": "2025-11-22T11:43:55.565548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a31ea5",
   "metadata": {
    "papermill": {
     "duration": 0.006475,
     "end_time": "2025-11-22T11:43:57.682459",
     "exception": false,
     "start_time": "2025-11-22T11:43:57.675984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: gemma2-9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2287c02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:57.697122Z",
     "iopub.status.busy": "2025-11-22T11:43:57.696841Z",
     "iopub.status.idle": "2025-11-22T11:43:57.705073Z",
     "shell.execute_reply": "2025-11-22T11:43:57.704215Z"
    },
    "papermill": {
     "duration": 0.017931,
     "end_time": "2025-11-22T11:43:57.706684",
     "exception": false,
     "start_time": "2025-11-22T11:43:57.688753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gemma.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gemma.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "from human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "# --- Configuration ---\n",
    "model_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"  # Pretrained model checkpoint path\n",
    "csv_path = \"test.parquet\"  # Path to input data in .parquet format\n",
    "\n",
    "# --- Load tokenizer and processor ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Processor to tokenize input samples (prompt-response pairs)\n",
    "processor = ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=False,\n",
    ")\n",
    "\n",
    "# Load dataset and apply preprocessing\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "\n",
    "# DataLoader with custom collator to batch examples based on total token count\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,  # Each \"batch\" is a list of micro-batches\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192,  # Maximum total tokens per batch\n",
    "        base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- Define pipeline parallelism across 2 GPUs ---\n",
    "\n",
    "# Total number of transformer layers in the model\n",
    "num_hidden_layers = 42\n",
    "\n",
    "# Assign embedding, final layers, and score head to GPU 0 and 1\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "\n",
    "# Split model layers between two GPUs: first half on cuda:0, second half on cuda:1\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "# Load model with weights onto corresponding devices and use float16\n",
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# --- Prepare rotary embeddings ---\n",
    "config = model.config\n",
    "dim = config.head_dim  # dimension of attention heads\n",
    "# Compute inverse frequencies for RoPE (rotary positional encoding)\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")  # For first half of model\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")  # For second half\n",
    "\n",
    "# --- Inference loop using pipelined execution ---\n",
    "is_first = True\n",
    "hidden_states = None\n",
    "outs = []\n",
    "\n",
    "# Loop through all batches\n",
    "for batch in tqdm(dataloader):\n",
    "    # Each batch is a list of micro-batches\n",
    "    for micro_batch in batch:\n",
    "        # Move input tokens to GPU 0\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "\n",
    "        # Prepare sequence-related information\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "\n",
    "        # If first iteration, run only part 1 and store state\n",
    "        if is_first:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n",
    "            is_first = False\n",
    "            # Move intermediate outputs to GPU 1 for next step\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Run part 2 for previous micro-batch and part 1 for current micro-batch\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            # Compute final logits for previous hidden states\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "\n",
    "            # Compute hidden states for the next micro-batch\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n",
    "\n",
    "            # Move new hidden state and seq_info to GPU 1\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())  # Store prediction logits on CPU\n",
    "\n",
    "# --- Final prediction for the last micro-batch ---\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "\n",
    "# Concatenate all logits and compute probabilities\n",
    "pred = torch.cat(outs, dim=0)\n",
    "prob = pred.softmax(-1)\n",
    "\n",
    "# Evaluate predictions with dataset's built-in method\n",
    "print(dataset.evaluate(prob.numpy()))\n",
    "\n",
    "# Save prediction probabilities to file\n",
    "np.save('prob_m0.npy', prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64219d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:43:57.720709Z",
     "iopub.status.busy": "2025-11-22T11:43:57.720457Z",
     "iopub.status.idle": "2025-11-22T11:47:42.276981Z",
     "shell.execute_reply": "2025-11-22T11:47:42.276082Z"
    },
    "papermill": {
     "duration": 224.565851,
     "end_time": "2025-11-22T11:47:42.279007",
     "exception": false,
     "start_time": "2025-11-22T11:43:57.713156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [03:17<00:00, 49.29s/it]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-11-22 11:47:25.329495: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-11-22 11:47:25.329611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-11-22 11:47:25.461325: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.80s/it]\r\n",
      "{'log_loss': 3.094615495202658}\r\n"
     ]
    }
   ],
   "source": [
    "!python gemma.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e7da3",
   "metadata": {
    "papermill": {
     "duration": 0.006754,
     "end_time": "2025-11-22T11:47:42.293102",
     "exception": false,
     "start_time": "2025-11-22T11:47:42.286348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: llama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9d986a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:47:42.308780Z",
     "iopub.status.busy": "2025-11-22T11:47:42.308499Z",
     "iopub.status.idle": "2025-11-22T11:47:42.315809Z",
     "shell.execute_reply": "2025-11-22T11:47:42.314964Z"
    },
    "papermill": {
     "duration": 0.017518,
     "end_time": "2025-11-22T11:47:42.317576",
     "exception": false,
     "start_time": "2025-11-22T11:47:42.300058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llama.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "from human_pref.models.modeling_llama import LlamaForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "# --- Configurations ---\n",
    "model_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"  # Path to pretrained LLaMA checkpoint\n",
    "csv_path = \"test_swap.parquet\"  # Dataset file (responses are swapped)\n",
    "\n",
    "# --- Tokenizer and Processor setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Suppress tokenizer length warning\n",
    "tokenizer.deprecation_warnings[\n",
    "    \"sequence-length-is-longer-than-the-specified-maximum\"\n",
    "] = True\n",
    "\n",
    "# Processor formats prompt-response input for the model\n",
    "processor = ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=True,\n",
    ")\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "\n",
    "# --- DataLoader setup with dynamic token batching ---\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192,\n",
    "        base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- Device mapping for pipelined parallelism ---\n",
    "num_hidden_layers = 32  # LLaMA-3 has 32 transformer layers\n",
    "\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "\n",
    "# First half of layers on GPU 0\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "\n",
    "# Second half of layers on GPU 1\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "# Load the model with float16 precision using the device map\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# --- Prepare rotary position encodings ---\n",
    "config = model.config\n",
    "dim = config.hidden_size // config.num_attention_heads\n",
    "\n",
    "# Compute inverse frequencies for rotary embeddings\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "# --- Pipelined Inference across 2 GPUs ---\n",
    "is_first = True  # Special case for first micro-batch\n",
    "hidden_states = None\n",
    "outs = []  # Store output logits\n",
    "\n",
    "# Loop through batches\n",
    "for batch in tqdm(dataloader):\n",
    "    for micro_batch in batch:\n",
    "        # Move inputs to GPU 0\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "\n",
    "        # Construct sequence info for attention mask and positional encoding\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "\n",
    "        if is_first:\n",
    "            # First micro-batch: run only forward_part1\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            is_first = False\n",
    "\n",
    "            # Move intermediate results to GPU 1 for next step\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Pipelined inference:\n",
    "        # - Run part2 for previous micro-batch on GPU 1\n",
    "        # - Run part1 for current micro-batch on GPU 0\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "\n",
    "            # Prepare next micro-batch state\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())  # Save logits to CPU memory\n",
    "\n",
    "# --- Process final micro-batch (no part1 needed) ---\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "# --- Evaluate and Save ---\n",
    "pred = torch.cat(outs, dim=0)         # Concatenate logits\n",
    "prob = pred.softmax(-1)               # Convert logits to probabilities\n",
    "print(dataset.evaluate(prob.numpy())) # Evaluate predictions\n",
    "\n",
    "np.save('prob_m3.npy', prob)          # Save prediction scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3584bf12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:47:42.332756Z",
     "iopub.status.busy": "2025-11-22T11:47:42.332544Z",
     "iopub.status.idle": "2025-11-22T11:50:33.709091Z",
     "shell.execute_reply": "2025-11-22T11:50:33.708229Z"
    },
    "papermill": {
     "duration": 171.386567,
     "end_time": "2025-11-22T11:50:33.711320",
     "exception": false,
     "start_time": "2025-11-22T11:47:42.324753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:37<00:00, 39.38s/it]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-11-22 11:50:25.141745: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-11-22 11:50:25.141813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-11-22 11:50:25.143589: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.08s/it]\r\n",
      "{'log_loss': 0.7582519183970864}\r\n"
     ]
    }
   ],
   "source": [
    "!python llama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dfac800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:33.728496Z",
     "iopub.status.busy": "2025-11-22T11:50:33.728090Z",
     "iopub.status.idle": "2025-11-22T11:50:33.734360Z",
     "shell.execute_reply": "2025-11-22T11:50:33.733356Z"
    },
    "papermill": {
     "duration": 0.016628,
     "end_time": "2025-11-22T11:50:33.735990",
     "exception": false,
     "start_time": "2025-11-22T11:50:33.719362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98613375 0.0021362  0.01173002]\n",
      " [0.13743691 0.5760938  0.28646934]\n",
      " [0.7586595  0.09096359 0.1503769 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "prob = np.load('prob_m3.npy')\n",
    "\n",
    "print(prob[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77443fcd",
   "metadata": {
    "papermill": {
     "duration": 0.007199,
     "end_time": "2025-11-22T11:50:33.750454",
     "exception": false,
     "start_time": "2025-11-22T11:50:33.743255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e59d753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:33.766461Z",
     "iopub.status.busy": "2025-11-22T11:50:33.766223Z",
     "iopub.status.idle": "2025-11-22T11:50:42.133714Z",
     "shell.execute_reply": "2025-11-22T11:50:42.132945Z"
    },
    "papermill": {
     "duration": 8.377971,
     "end_time": "2025-11-22T11:50:42.135654",
     "exception": false,
     "start_time": "2025-11-22T11:50:33.757683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 11:50:37.048375: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-22 11:50:37.048459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-22 11:50:37.050236: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import faiss\n",
    "\n",
    "\n",
    "\n",
    "model_load_path = '/kaggle/input/some-pack/sentence-transformer-model' \n",
    "sentence_model = SentenceTransformer(model_load_path)\n",
    "\n",
    "test_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n",
    "\n",
    "# Note: CustomDebertaModel class definition removed as it's not needed for inference.\n",
    "# The model is loaded from a pre-trained checkpoint directly using torch.load()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import faiss\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove any HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove special characters that are not useful\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"-]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDebertaModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, feature_dim=2, dropout_rate=0.1):\n",
    "        super(CustomDebertaModel, self).__init__()\n",
    "        \n",
    "        # Initialize DeBERTa model\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Feature tower for similarity features (a small MLP)\n",
    "        self.feature_fc = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),                 # Map input similarity features to 128-dimensional space\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, self.base_model.config.hidden_size),  # Project to same size as text embeddings\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism to allow interaction between text and similarity embeddings\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.base_model.config.hidden_size,\n",
    "            num_heads=4,  # Number of attention heads\n",
    "            batch_first=True  # Enable batch-first input format\n",
    "        )\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Final classifier layer (MLP for classification)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.base_model.config.hidden_size * 2, self.base_model.config.hidden_size),  # Combine text + attention features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, similarity_features, labels=None):\n",
    "        # Text tower: extract [CLS] token embedding from DeBERTa\n",
    "        base_outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = base_outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "        # Feature tower: process similarity features through MLP\n",
    "        similarity_embeds = self.feature_fc(similarity_features)  # [batch_size, hidden_size]\n",
    "\n",
    "        # Cross-modal interaction using attention mechanism\n",
    "        query = text_embeddings.unsqueeze(1)       # Shape: [batch_size, 1, hidden_size]\n",
    "        key_value = similarity_embeds.unsqueeze(1) # Shape: [batch_size, 1, hidden_size]\n",
    "        attention_output, _ = self.attention(query, key_value, key_value)  # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # Concatenate text and attended similarity features\n",
    "        combined_features = torch.cat([text_embeddings, attention_output.squeeze(1)], dim=1)\n",
    "\n",
    "        # Apply dropout and classification head\n",
    "        logits = self.classifier(self.dropout(combined_features))\n",
    "\n",
    "        # Output dictionary with logits\n",
    "        outputs = {\"logits\": logits}\n",
    "        \n",
    "        # If labels are provided (e.g., during training), compute the cross-entropy loss\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            outputs[\"loss\"] = loss_fn(logits, labels)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c77a511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:42.152687Z",
     "iopub.status.busy": "2025-11-22T11:50:42.152412Z",
     "iopub.status.idle": "2025-11-22T11:50:42.163404Z",
     "shell.execute_reply": "2025-11-22T11:50:42.162475Z"
    },
    "papermill": {
     "duration": 0.021408,
     "end_time": "2025-11-22T11:50:42.165139",
     "exception": false,
     "start_time": "2025-11-22T11:50:42.143731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import faiss\n",
    "\n",
    "test_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n",
    "\n",
    "# Compute semantic similarity scores using FAISS\n",
    "def compute_semantic_features_with_faiss(df):\n",
    "    # Clean the text data\n",
    "    df['prompt'] = df['prompt'].apply(clean_text)\n",
    "    df['response_a'] = df['response_a'].apply(clean_text)\n",
    "    df['response_b'] = df['response_b'].apply(clean_text)\n",
    "\n",
    "    # Extract prompts and responses as lists\n",
    "    prompts = df['prompt'].tolist()\n",
    "    responses_a = df['response_a'].tolist()\n",
    "    responses_b = df['response_b'].tolist()\n",
    "\n",
    "    # Generate sentence embeddings and normalize them (unit vectors)\n",
    "    prompt_embeddings = np.array(sentence_model.encode(prompts, show_progress=False))\n",
    "    prompt_embeddings = prompt_embeddings / np.linalg.norm(prompt_embeddings, axis=1, keepdims=True) if prompt_embeddings.shape[0] > 0 else prompt_embeddings\n",
    "\n",
    "    response_a_embeddings = np.array(sentence_model.encode(responses_a,show_progress=False))\n",
    "    response_a_embeddings = response_a_embeddings / np.linalg.norm(response_a_embeddings, axis=1, keepdims=True) if response_a_embeddings.shape[0] > 0 else response_a_embeddings\n",
    "\n",
    "    response_b_embeddings = np.array(sentence_model.encode(responses_b,show_progress=False))\n",
    "    response_b_embeddings = response_b_embeddings / np.linalg.norm(response_b_embeddings, axis=1, keepdims=True) if response_b_embeddings.shape[0] > 0 else response_b_embeddings\n",
    "\n",
    "    # Determine the embedding dimension\n",
    "    dim = prompt_embeddings.shape[1]\n",
    "\n",
    "    # Create a FAISS index using inner product (cosine similarity since vectors are normalized)\n",
    "    index_flat = faiss.IndexFlatIP(dim)\n",
    "\n",
    "    # Compute similarity between response A and prompt\n",
    "    index_flat.add(prompt_embeddings)  # Add prompt embeddings to the FAISS index\n",
    "    similarity_a = index_flat.search(response_a_embeddings, k=1)[0].squeeze()  # Get top-1 similarity score\n",
    "\n",
    "    # Reset and compute similarity for response B\n",
    "    index_flat.reset()\n",
    "    index_flat.add(prompt_embeddings)\n",
    "    similarity_b = index_flat.search(response_b_embeddings, k=1)[0].squeeze()\n",
    "\n",
    "    # Additional feature: response length ratio\n",
    "    df['length_ratio'] = df['response_a'].str.len() / (df['response_b'].str.len() + 1e-8)\n",
    "\n",
    "    # Additional feature: punctuation density\n",
    "    df['punct_a_density'] = df['response_a'].str.count(r'[.,!?]') / (df['response_a'].str.len() + 1e-8)\n",
    "    df['punct_b_density'] = df['response_b'].str.count(r'[.,!?]') / (df['response_b'].str.len() + 1e-8)\n",
    "\n",
    "    # Additional feature: similarity difference\n",
    "    df['similarity_diff'] = abs(similarity_a - similarity_b)\n",
    "\n",
    "    # Store similarity scores in the DataFrame\n",
    "    df['similarity_a'] = similarity_a\n",
    "    df['similarity_b'] = similarity_b\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bdd9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:42.182612Z",
     "iopub.status.busy": "2025-11-22T11:50:42.182379Z",
     "iopub.status.idle": "2025-11-22T11:50:43.047759Z",
     "shell.execute_reply": "2025-11-22T11:50:43.046818Z"
    },
    "papermill": {
     "duration": 0.875858,
     "end_time": "2025-11-22T11:50:43.049680",
     "exception": false,
     "start_time": "2025-11-22T11:50:42.173822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a41b5970b7440eb85ef72e80bd3348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d25b153e3d5498791332e45427919e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e313a770a846bf872b16c64e9e53ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = compute_semantic_features_with_faiss(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8b4a770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:43.067917Z",
     "iopub.status.busy": "2025-11-22T11:50:43.067655Z",
     "iopub.status.idle": "2025-11-22T11:50:48.468120Z",
     "shell.execute_reply": "2025-11-22T11:50:48.467181Z"
    },
    "papermill": {
     "duration": 5.411022,
     "end_time": "2025-11-22T11:50:48.469977",
     "exception": false,
     "start_time": "2025-11-22T11:50:43.058955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the trained custom PyTorch model\n",
    "model = torch.load(\"/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\")\n",
    "\n",
    "# Set device to GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Important: set to evaluation mode\n",
    "\n",
    "print(\"Custom model loaded successfully!\")\n",
    "\n",
    "# Load the tokenizer used during training\n",
    "tokenizer_path = \"/kaggle/input/akemiiiiii/custom_model_dir\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Function to preprocess a single test sample\n",
    "def preprocess_test_data(row):\n",
    "    # Construct the input string in the same format used for training\n",
    "    input_text = f\"Prompt: {row['prompt']} Response A: {row['response_a']} Response B: {row['response_b']}\"\n",
    "\n",
    "    # Tokenize the input using the same tokenizer settings as training\n",
    "    tokenized_inputs = tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    # Add similarity scores as additional features\n",
    "    # They should be shaped as [1, feature_dim] to batch correctly\n",
    "    similarity = torch.tensor([[row[\"similarity_a\"], row[\"similarity_b\"]]], dtype=torch.float32)\n",
    "\n",
    "    tokenized_inputs[\"similarity_features\"] = similarity\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the preprocessing function to every row of the test dataset\n",
    "processed_test_data = [preprocess_test_data(row) for _, row in test_data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c64a0d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:48.489240Z",
     "iopub.status.busy": "2025-11-22T11:50:48.488753Z",
     "iopub.status.idle": "2025-11-22T11:50:48.495129Z",
     "shell.execute_reply": "2025-11-22T11:50:48.494328Z"
    },
    "papermill": {
     "duration": 0.017165,
     "end_time": "2025-11-22T11:50:48.497026",
     "exception": false,
     "start_time": "2025-11-22T11:50:48.479861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "test_dataset = TestDataset(processed_test_data)\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    # Concatenate input_ids and attention_mask along batch dimension\n",
    "    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)\n",
    "    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)\n",
    "    \n",
    "    # Stack similarity features (already shape [1, 2])\n",
    "    similarity_features = torch.cat([item[\"similarity_features\"] for item in batch], dim=0)  # shape [B, 2]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"similarity_features\": similarity_features,\n",
    "    }\n",
    "\n",
    "# DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f5ce585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:48.514788Z",
     "iopub.status.busy": "2025-11-22T11:50:48.514186Z",
     "iopub.status.idle": "2025-11-22T11:50:48.839861Z",
     "shell.execute_reply": "2025-11-22T11:50:48.838997Z"
    },
    "papermill": {
     "duration": 0.33653,
     "end_time": "2025-11-22T11:50:48.841889",
     "exception": false,
     "start_time": "2025-11-22T11:50:48.505359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4804413  0.5107934  0.00876523]\n",
      " [0.41196185 0.36425284 0.22378537]\n",
      " [0.4751581  0.517468   0.00737392]]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode (important to deactivate dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "# List to store prediction results\n",
    "predictions = []\n",
    "\n",
    "# Disable gradient calculation (saves memory and speeds up inference)\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Move batch data to the correct device (GPU or CPU)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        similarity_features = batch[\"similarity_features\"].to(device)\n",
    "\n",
    "        # Run forward pass (model inference)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            similarity_features=similarity_features\n",
    "        )\n",
    "\n",
    "        # Extract raw logits from output\n",
    "        logits = outputs[\"logits\"]\n",
    "\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Move to CPU and convert to NumPy for later use\n",
    "        predictions.append(probs.cpu().numpy())\n",
    "\n",
    "# Concatenate all batch results into a single NumPy array\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Print the final predictions\n",
    "print(predictions)\n",
    "\n",
    "# Save predictions to a .npy file for downstream analysis or evaluation\n",
    "np.save('prob_faiss.npy', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b5c48",
   "metadata": {
    "papermill": {
     "duration": 0.007842,
     "end_time": "2025-11-22T11:50:48.858264",
     "exception": false,
     "start_time": "2025-11-22T11:50:48.850422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ddea9c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T11:50:48.875564Z",
     "iopub.status.busy": "2025-11-22T11:50:48.875275Z",
     "iopub.status.idle": "2025-11-22T11:50:48.929007Z",
     "shell.execute_reply": "2025-11-22T11:50:48.928079Z"
    },
    "papermill": {
     "duration": 0.064597,
     "end_time": "2025-11-22T11:50:48.930920",
     "exception": false,
     "start_time": "2025-11-22T11:50:48.866323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.052022        0.923323    0.024655\n",
      "1   211333        0.483411        0.163534    0.353055\n",
      "2  1233961        0.138111        0.708878    0.153010\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "\n",
    "prob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\n",
    "prob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3 (swap response_a and response_b)\n",
    "prob_faiss = np.load(\"prob_faiss.npy\")  # faiss\n",
    "\n",
    "# Calibrate probabilities with temperature scaling for better confidence\n",
    "def calibrate_probabilities(probs, temperature=1.0):\n",
    "    # Apply temperature scaling\n",
    "    calibrated = probs ** (1/temperature)\n",
    "    # Normalize to ensure sum to 1\n",
    "    return calibrated / np.sum(calibrated, axis=1, keepdims=True)\n",
    "\n",
    "prob_m0_calib = calibrate_probabilities(prob_m0, temperature=1.1)\n",
    "prob_m3_calib = calibrate_probabilities(prob_m3, temperature=1.2)\n",
    "prob_faiss_calib = calibrate_probabilities(prob_faiss, temperature=1.0)\n",
    "\n",
    "def compute_entropy(probs):\n",
    "    return -np.sum(probs * np.log(probs + 1e-12), axis=1)\n",
    "\n",
    "# Compute entropy (lower entropy = higher confidence)\n",
    "entropy_m0 = compute_entropy(prob_m0_calib)\n",
    "entropy_m3 = compute_entropy(prob_m3_calib)\n",
    "entropy_faiss = compute_entropy(prob_faiss_calib)\n",
    "\n",
    "weights = np.array([\n",
    "    1.0 / (entropy_m0 + 1e-8),\n",
    "    1.0 / (entropy_m3 + 1e-8),\n",
    "    1.0 / (entropy_faiss + 1e-8)\n",
    "])\n",
    "weights = weights / np.sum(weights, axis=0)\n",
    "\n",
    "# Combine predictions with weights\n",
    "# Adjust weights as needed for optimal performance\n",
    "preds = np.average(\n",
    "    [\n",
    "        prob_m0_calib,  # Calibrated Gemma2 results\n",
    "        prob_m3_calib,  # Calibrated Llama3 results\n",
    "        prob_faiss_calib  # Calibrated faiss results\n",
    "    ],\n",
    "    axis=0,\n",
    "    weights=[0.7, 0.2, 0.1]  # Base weights can be combined with dynamic weights\n",
    "    # For true dynamic weighting, replace with weights=[weights[0, i] for each sample]\n",
    "    # But for simplicity, we'll keep the base weights and add temperature scaling\n",
    ")\n",
    "\n",
    "# Create submission DataFrame\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": df[\"id\"],\n",
    "    \"winner_model_a\": preds[:, 0],\n",
    "    \"winner_model_b\": preds[:, 1],\n",
    "    \"winner_tie\": preds[:, 2],\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(sub.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1e6e",
   "metadata": {
    "papermill": {
     "duration": 0.008646,
     "end_time": "2025-11-22T11:50:48.948388",
     "exception": false,
     "start_time": "2025-11-22T11:50:48.939742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference\n",
    "- LMSYS - Chatbot Arena Human Preference Predictions 2nd place solution - https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685\n",
    "- Blue - https://www.kaggle.com/code/blue0924/finetuning-test2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5493674,
     "sourceId": 9102725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496762,
     "sourceId": 9107824,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496847,
     "sourceId": 9107963,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496920,
     "sourceId": 9108069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6267026,
     "sourceId": 10214229,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6265823,
     "sourceId": 10302322,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 563.321488,
   "end_time": "2025-11-22T11:50:51.797900",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-22T11:41:28.476412",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07b7fd3b3d3d48adb6f6cf5b6b4b2485": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ee9e1ee1807840968a09a1406cfbe2a8",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b2c4e72b18e545829d9d62ec07560eb6",
       "value": 1.0
      }
     },
     "0b48e46e97994077b8f2ce7764e1771e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ecf8cfd056a747cca2c509e481ce41fe",
       "placeholder": "​",
       "style": "IPY_MODEL_75a4d444d73b418384a719a9f7c81628",
       "value": " 1/1 [00:00&lt;00:00,  1.27it/s]"
      }
     },
     "0efc14ff22d9450b889f39e4b5c6587b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "104281b1791d44e4aeaf630984d3fc91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "129b73ef18e84104a4573f740be641ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_104281b1791d44e4aeaf630984d3fc91",
       "placeholder": "​",
       "style": "IPY_MODEL_0efc14ff22d9450b889f39e4b5c6587b",
       "value": "Batches: 100%"
      }
     },
     "1a9ce9c7c8004ef180e90f6c449d250f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1d25b153e3d5498791332e45427919e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_129b73ef18e84104a4573f740be641ee",
        "IPY_MODEL_e2cf506b84ca41fb919c49fbc1303f69",
        "IPY_MODEL_a9b1f29900994b4f806a686a7d6811a9"
       ],
       "layout": "IPY_MODEL_82526c83b232462db9f5005bc151f00a"
      }
     },
     "20f4eaa7d0074937a76bcfa45ed13cfe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "23fe01ac0b38435fbfffc9d82047dee8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d89e25511ba641ceb320bfbd345bcfe7",
       "placeholder": "​",
       "style": "IPY_MODEL_1a9ce9c7c8004ef180e90f6c449d250f",
       "value": "Batches: 100%"
      }
     },
     "27a74c75c2b0430b94aa92698986ebf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2a0265e2fd0649be8953c8258e1f713e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41cb1e3472054ec8aa060d04aa4ccda9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2a0265e2fd0649be8953c8258e1f713e",
       "placeholder": "​",
       "style": "IPY_MODEL_bbbe41c9fb91445986ec2b3889f52554",
       "value": " 1/1 [00:00&lt;00:00, 37.26it/s]"
      }
     },
     "46c8eaf2d5e440618b94ae916b9b221b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "58e313a770a846bf872b16c64e9e53ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_96cd6d179b9348e3a5124e82e99bca9d",
        "IPY_MODEL_803c4c4295a6411d931e3d5675445885",
        "IPY_MODEL_41cb1e3472054ec8aa060d04aa4ccda9"
       ],
       "layout": "IPY_MODEL_6bc557b59a5a4869a85845e3443ca616"
      }
     },
     "64dba67b59bc4e5eb062267c0d22aebf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6bc557b59a5a4869a85845e3443ca616": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75a4d444d73b418384a719a9f7c81628": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c7eda34ea884962be00baff79cb7bc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "803c4c4295a6411d931e3d5675445885": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64dba67b59bc4e5eb062267c0d22aebf",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b10f4f7789cd4cdeae47f4c040449510",
       "value": 1.0
      }
     },
     "812dd1beceb640e097f05261ad388d9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82526c83b232462db9f5005bc151f00a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96cd6d179b9348e3a5124e82e99bca9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cd3910044a374269974f977c4924892a",
       "placeholder": "​",
       "style": "IPY_MODEL_46c8eaf2d5e440618b94ae916b9b221b",
       "value": "Batches: 100%"
      }
     },
     "a9b1f29900994b4f806a686a7d6811a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_812dd1beceb640e097f05261ad388d9f",
       "placeholder": "​",
       "style": "IPY_MODEL_cd7662b0a0934399a3458726e59296f5",
       "value": " 1/1 [00:00&lt;00:00, 38.33it/s]"
      }
     },
     "b10f4f7789cd4cdeae47f4c040449510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b2c4e72b18e545829d9d62ec07560eb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bbbe41c9fb91445986ec2b3889f52554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cd3910044a374269974f977c4924892a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd7662b0a0934399a3458726e59296f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d89e25511ba641ceb320bfbd345bcfe7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e2cf506b84ca41fb919c49fbc1303f69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_20f4eaa7d0074937a76bcfa45ed13cfe",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_27a74c75c2b0430b94aa92698986ebf2",
       "value": 1.0
      }
     },
     "e4a41b5970b7440eb85ef72e80bd3348": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_23fe01ac0b38435fbfffc9d82047dee8",
        "IPY_MODEL_07b7fd3b3d3d48adb6f6cf5b6b4b2485",
        "IPY_MODEL_0b48e46e97994077b8f2ce7764e1771e"
       ],
       "layout": "IPY_MODEL_7c7eda34ea884962be00baff79cb7bc2"
      }
     },
     "ecf8cfd056a747cca2c509e481ce41fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee9e1ee1807840968a09a1406cfbe2a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
